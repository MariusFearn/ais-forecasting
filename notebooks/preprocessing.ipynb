{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef44be59",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline\n",
    "\n",
    "This notebook demonstrates the complete data preprocessing pipeline for AIS data.\n",
    "\n",
    "## Contents\n",
    "1. Data Loading and Validation\n",
    "2. Data Cleaning\n",
    "3. Geospatial Feature Engineering\n",
    "4. Temporal Feature Engineering\n",
    "5. Data Aggregation and Time Series Creation\n",
    "6. Feature Selection and Normalization\n",
    "7. Dataset Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae8d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from data.loader import AISDataLoader\n",
    "from data.preprocessing import AISDataPreprocessor\n",
    "from features.geo_features import GeoFeatureEngineer\n",
    "from features.time_features import TimeFeatureEngineer\n",
    "from visualization.plots import setup_plot_style\n",
    "\n",
    "# Set up plotting\n",
    "setup_plot_style()\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe9d0df",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cf4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "data_loader = AISDataLoader('../data')\n",
    "preprocessor = AISDataPreprocessor()\n",
    "geo_engineer = GeoFeatureEngineer(h3_resolution=8)\n",
    "time_engineer = TimeFeatureEngineer()\n",
    "\n",
    "print(\"Preprocessing pipeline initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca73c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data (replace with your actual data file)\n",
    "# df_raw = data_loader.load_raw_data('../data/raw/your_ais_data.csv')\n",
    "\n",
    "# For demo purposes, create sample data\n",
    "np.random.seed(42)\n",
    "n_samples = 50000\n",
    "n_vessels = 100\n",
    "vessel_ids = [f'V{i:03d}' for i in range(1, n_vessels + 1)]\n",
    "\n",
    "# Create realistic vessel tracks\n",
    "df_raw = pd.DataFrame({\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=n_samples, freq='5min'),\n",
    "    'vessel_id': np.random.choice(vessel_ids, n_samples),\n",
    "    'lat': np.random.uniform(58, 62, n_samples),  # Norwegian waters\n",
    "    'lon': np.random.uniform(4, 12, n_samples),\n",
    "    'speed': np.random.exponential(8, n_samples),  # Speed in knots\n",
    "    'course': np.random.uniform(0, 360, n_samples),  # Course in degrees\n",
    "})\n",
    "\n",
    "# Add some data quality issues for demonstration\n",
    "# Missing values\n",
    "missing_idx = np.random.choice(df_raw.index, size=int(0.02 * len(df_raw)), replace=False)\n",
    "df_raw.loc[missing_idx[:len(missing_idx)//2], 'speed'] = np.nan\n",
    "df_raw.loc[missing_idx[len(missing_idx)//2:], 'course'] = np.nan\n",
    "\n",
    "# Invalid coordinates\n",
    "invalid_idx = np.random.choice(df_raw.index, size=100, replace=False)\n",
    "df_raw.loc[invalid_idx[:50], 'lat'] = 95  # Invalid latitude\n",
    "df_raw.loc[invalid_idx[50:], 'lon'] = 185  # Invalid longitude\n",
    "\n",
    "print(f\"Raw dataset shape: {df_raw.shape}\")\n",
    "print(f\"Date range: {df_raw['timestamp'].min()} to {df_raw['timestamp'].max()}\")\n",
    "print(f\"Number of vessels: {df_raw['vessel_id'].nunique()}\")\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc58e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate raw data\n",
    "validation_issues = preprocessor.validate_ais_data(df_raw)\n",
    "\n",
    "print(\"Data Validation Results:\")\n",
    "if validation_issues:\n",
    "    for issue in validation_issues:\n",
    "        print(f\"  ⚠️  {issue}\")\n",
    "else:\n",
    "    print(\"  ✅ No validation issues found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16132d1a",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b2483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "print(\"Cleaning data...\")\n",
    "df_clean = preprocessor.clean_ais_data(\n",
    "    df_raw, \n",
    "    remove_outliers=True, \n",
    "    speed_threshold=50.0\n",
    ")\n",
    "\n",
    "print(f\"\\nData cleaning results:\")\n",
    "print(f\"  Original records: {len(df_raw):,}\")\n",
    "print(f\"  Clean records: {len(df_clean):,}\")\n",
    "print(f\"  Removed: {len(df_raw) - len(df_clean):,} ({((len(df_raw) - len(df_clean)) / len(df_raw) * 100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce98e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle remaining missing values\n",
    "print(\"Handling missing values...\")\n",
    "df_clean = preprocessor.handle_missing_values(\n",
    "    df_clean, \n",
    "    strategy='interpolate', \n",
    "    columns=['speed', 'course']\n",
    ")\n",
    "\n",
    "# Check for remaining missing values\n",
    "missing_after = df_clean.isnull().sum()\n",
    "print(f\"\\nMissing values after cleaning:\")\n",
    "print(missing_after[missing_after > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc4173a",
   "metadata": {},
   "source": [
    "## 3. Geospatial Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac13c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create H3 geospatial features\n",
    "print(\"Creating H3 geospatial features...\")\n",
    "df_geo = geo_engineer.create_h3_cells(df_clean)\n",
    "df_geo = geo_engineer.create_h3_center_coordinates(df_geo)\n",
    "\n",
    "print(f\"Sample H3 cells:\")\n",
    "print(df_geo[['lat', 'lon', 'h3_cell', 'h3_center_lat', 'h3_center_lon']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c3b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distance and speed features\n",
    "print(\"Calculating distance and speed features...\")\n",
    "df_geo = geo_engineer.calculate_distance_features(df_geo, group_col='vessel_id')\n",
    "df_geo = geo_engineer.create_speed_features(df_geo, group_col='vessel_id')\n",
    "df_geo = geo_engineer.create_bearing_features(df_geo, group_col='vessel_id')\n",
    "\n",
    "print(f\"Geospatial features created:\")\n",
    "geo_cols = ['distance_from_previous', 'cumulative_distance', 'calculated_speed', 'bearing']\n",
    "print(df_geo[geo_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb9974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize H3 cells\n",
    "h3_counts = df_geo['h3_cell'].value_counts().head(20)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(h3_counts)), h3_counts.values)\n",
    "plt.title('Top 20 H3 Cells by Observation Count')\n",
    "plt.xlabel('H3 Cell Rank')\n",
    "plt.ylabel('Number of Observations')\n",
    "plt.xticks(range(len(h3_counts)), [f'Cell {i+1}' for i in range(len(h3_counts))], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total unique H3 cells: {df_geo['h3_cell'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3116e55",
   "metadata": {},
   "source": [
    "## 4. Temporal Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8080750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal features\n",
    "print(\"Creating temporal features...\")\n",
    "df_temporal = time_engineer.create_basic_time_features(df_geo, 'timestamp')\n",
    "df_temporal = time_engineer.create_cyclical_features(df_temporal, 'timestamp')\n",
    "df_temporal = time_engineer.create_seasonal_features(df_temporal, 'timestamp')\n",
    "\n",
    "print(\"Temporal features created:\")\n",
    "temporal_cols = ['hour', 'day_of_week', 'month', 'hour_sin', 'hour_cos', 'is_weekend', 'season']\n",
    "print(df_temporal[temporal_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ccda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag and rolling features for speed (as an example target variable)\n",
    "print(\"Creating lag and rolling features...\")\n",
    "df_temporal = time_engineer.create_lag_features(\n",
    "    df_temporal, \n",
    "    target_col='speed',\n",
    "    group_cols=['vessel_id'],\n",
    "    lags=[1, 2, 3, 6, 12]  # 1, 2, 3, 6, 12 time steps back\n",
    ")\n",
    "\n",
    "df_temporal = time_engineer.create_rolling_features(\n",
    "    df_temporal,\n",
    "    target_col='speed',\n",
    "    group_cols=['vessel_id'],\n",
    "    windows=[3, 6, 12, 24],\n",
    "    features=['mean', 'std', 'max']\n",
    ")\n",
    "\n",
    "print(\"Lag and rolling features created.\")\n",
    "lag_cols = [col for col in df_temporal.columns if 'lag' in col or 'rolling' in col]\n",
    "print(f\"Number of lag/rolling features: {len(lag_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7905ab6",
   "metadata": {},
   "source": [
    "## 5. Data Aggregation and Time Series Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd867807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time index for forecasting\n",
    "print(\"Creating time index...\")\n",
    "df_indexed = preprocessor.create_time_index(df_temporal, freq='H')  # Hourly aggregation\n",
    "\n",
    "print(f\"Time index range: {df_indexed['time_idx'].min()} to {df_indexed['time_idx'].max()}\")\n",
    "print(f\"Number of unique time steps: {df_indexed['time_idx'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc7506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to H3 grid\n",
    "print(\"Aggregating to H3 grid...\")\n",
    "df_grid = preprocessor.aggregate_to_grid(\n",
    "    df_indexed,\n",
    "    h3_col='h3_cell',\n",
    "    time_col='time_idx',\n",
    "    value_cols=['speed', 'calculated_speed'],\n",
    "    agg_functions={'speed': 'mean', 'calculated_speed': 'mean'}\n",
    ")\n",
    "\n",
    "print(f\"Grid aggregation results:\")\n",
    "print(f\"  Shape: {df_grid.shape}\")\n",
    "print(f\"  Unique H3 cells: {df_grid['h3_cell'].nunique()}\")\n",
    "print(f\"  Time steps: {df_grid['time_idx'].nunique()}\")\n",
    "\n",
    "df_grid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e75cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid aggregation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Vessel count distribution\n",
    "axes[0].hist(df_grid['vessel_count'], bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Distribution of Vessel Counts per Grid Cell')\n",
    "axes[0].set_xlabel('Vessel Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Average speed distribution\n",
    "axes[1].hist(df_grid['speed'].dropna(), bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Distribution of Average Speeds per Grid Cell')\n",
    "axes[1].set_xlabel('Average Speed (knots)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8643f21",
   "metadata": {},
   "source": [
    "## 6. Feature Selection and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdde1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final dataset for modeling\n",
    "print(\"Preparing final dataset...\")\n",
    "\n",
    "# Select features for modeling\n",
    "feature_columns = [\n",
    "    'h3_cell', 'time_idx', 'vessel_count', 'speed',\n",
    "    'hour', 'day_of_week', 'month', 'is_weekend',\n",
    "    'hour_sin', 'hour_cos', 'month_sin', 'month_cos'\n",
    "]\n",
    "\n",
    "# Add lag features if they exist\n",
    "lag_features = [col for col in df_grid.columns if 'speed_lag' in col]\n",
    "feature_columns.extend(lag_features[:5])  # Use first 5 lag features\n",
    "\n",
    "# Filter to only existing columns\n",
    "available_features = [col for col in feature_columns if col in df_grid.columns]\n",
    "df_model = df_grid[available_features].copy()\n",
    "\n",
    "print(f\"Selected features: {available_features}\")\n",
    "print(f\"Final dataset shape: {df_model.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90784120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in final dataset\n",
    "print(\"Final missing value check:\")\n",
    "missing_final = df_model.isnull().sum()\n",
    "print(missing_final[missing_final > 0])\n",
    "\n",
    "# Drop rows with missing target values\n",
    "df_model = df_model.dropna(subset=['speed'])\n",
    "\n",
    "# Fill remaining missing values\n",
    "numeric_cols = df_model.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if df_model[col].isnull().sum() > 0:\n",
    "        df_model[col] = df_model[col].fillna(df_model[col].median())\n",
    "\n",
    "print(f\"Final dataset shape after cleaning: {df_model.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d20dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical features\n",
    "print(\"Normalizing features...\")\n",
    "normalize_cols = ['vessel_count', 'speed'] + [col for col in df_model.columns if 'lag' in col]\n",
    "normalize_cols = [col for col in normalize_cols if col in df_model.columns]\n",
    "\n",
    "df_normalized, norm_params = preprocessor.normalize_features(\n",
    "    df_model, \n",
    "    columns=normalize_cols, \n",
    "    method='minmax'\n",
    ")\n",
    "\n",
    "print(f\"Normalized columns: {list(norm_params.keys())}\")\n",
    "print(\"Normalization parameters saved for inverse transformation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071e7bd",
   "metadata": {},
   "source": [
    "## 7. Dataset Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e9248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create group IDs for time series modeling\n",
    "df_final = df_normalized.copy()\n",
    "df_final['GroupIDS'] = df_final['h3_cell']  # Use H3 cell as group identifier\n",
    "\n",
    "# Rename target column\n",
    "df_final['value'] = df_final['speed']  # Standard target name\n",
    "\n",
    "# Ensure proper data types\n",
    "df_final['time_idx'] = df_final['time_idx'].astype(int)\n",
    "df_final['hour'] = df_final['hour'].astype(int)\n",
    "df_final['day_of_week'] = df_final['day_of_week'].astype(int)\n",
    "df_final['month'] = df_final['month'].astype(int)\n",
    "df_final['is_weekend'] = df_final['is_weekend'].astype(int)\n",
    "\n",
    "print(f\"Final dataset for modeling:\")\n",
    "print(f\"  Shape: {df_final.shape}\")\n",
    "print(f\"  Columns: {list(df_final.columns)}\")\n",
    "print(f\"  Time range: {df_final['time_idx'].min()} to {df_final['time_idx'].max()}\")\n",
    "print(f\"  Number of groups: {df_final['GroupIDS'].nunique()}\")\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c473bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final data quality check\n",
    "print(\"Final data quality assessment:\")\n",
    "print(f\"  Missing values: {df_final.isnull().sum().sum()}\")\n",
    "print(f\"  Duplicate rows: {df_final.duplicated().sum()}\")\n",
    "print(f\"  Data types:\")\n",
    "print(df_final.dtypes)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nFinal dataset statistics:\")\n",
    "print(df_final.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "print(\"Saving processed data...\")\n",
    "data_loader.save_processed_data(df_final, 'processed_ais_data')\n",
    "\n",
    "# Save normalization parameters\n",
    "import pickle\n",
    "with open('../data/processed/normalization_params.pkl', 'wb') as f:\n",
    "    pickle.dump(norm_params, f)\n",
    "\n",
    "print(\"✅ Preprocessing complete!\")\n",
    "print(f\"   Processed data saved as: processed_ais_data.pkl\")\n",
    "print(f\"   Normalization parameters saved as: normalization_params.pkl\")\n",
    "print(f\"   Ready for model training with {len(df_final):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e699fc",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The preprocessing pipeline successfully:\n",
    "\n",
    "1. ✅ **Loaded and validated** raw AIS data\n",
    "2. ✅ **Cleaned data** by removing invalid records and outliers\n",
    "3. ✅ **Created geospatial features** using H3 hexagonal grids\n",
    "4. ✅ **Engineered temporal features** including cyclical encoding\n",
    "5. ✅ **Aggregated data** to regular time intervals and spatial grids\n",
    "6. ✅ **Normalized features** for model training\n",
    "7. ✅ **Prepared final dataset** in the format required for forecasting models\n",
    "\n",
    "The processed data is now ready for training deep learning forecasting models like TFT and N-BEATS.\n",
    "\n",
    "**Next Steps:**\n",
    "- Train forecasting models using the prepared dataset\n",
    "- Evaluate model performance and tune hyperparameters\n",
    "- Deploy the best-performing model for production use"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
