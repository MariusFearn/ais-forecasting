{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48fdc52c",
   "metadata": {},
   "source": [
    "# AIS Forecasting - Model Development\n",
    "\n",
    "This notebook covers the model development pipeline for AIS trajectory forecasting.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Configuration\n",
    "2. Data Loading\n",
    "3. Model Configuration\n",
    "4. Training Pipeline\n",
    "5. Model Comparison\n",
    "6. Hyperparameter Tuning\n",
    "7. Model Evaluation\n",
    "8. Save Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f56228",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfab76f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root / 'src'))\n",
    "\n",
    "from src.data.loader import AISDataLoader\n",
    "from src.data.preprocessing import AISDataPreprocessor\n",
    "from src.models.tft_model import TFTModel\n",
    "from src.models.nbeats_model import NBeatsModel\n",
    "from src.utils.metrics import calculate_metrics\n",
    "from src.utils.optimize import OptunaOptimizer\n",
    "from src.visualization.plots import plot_forecast, plot_training_history\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "pl.seed_everything(42)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b40ba1",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bafbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load default configuration\n",
    "config_path = project_root / 'config' / 'default.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"- Data path: {config['data']['processed_path']}\")\n",
    "print(f\"- Model output: {config['model']['output_dir']}\")\n",
    "print(f\"- Sequence length: {config['model']['sequence_length']}\")\n",
    "print(f\"- Prediction horizon: {config['model']['prediction_horizon']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060205f4",
   "metadata": {},
   "source": [
    "## 3. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf7a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader and preprocessor\n",
    "data_loader = AISDataLoader(config)\n",
    "preprocessor = AISDataPreprocessor(config)\n",
    "\n",
    "# Note: In a real scenario, you would load actual AIS data\n",
    "# For demonstration, we'll create synthetic data\n",
    "print(\"Loading preprocessed data...\")\n",
    "\n",
    "# Load or create sample data\n",
    "try:\n",
    "    # Try to load existing processed data\n",
    "    train_data = pd.read_parquet(project_root / config['data']['processed_path'] / 'train.parquet')\n",
    "    val_data = pd.read_parquet(project_root / config['data']['processed_path'] / 'val.parquet')\n",
    "    test_data = pd.read_parquet(project_root / config['data']['processed_path'] / 'test.parquet')\n",
    "    print(f\"Loaded existing data: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test samples\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No processed data found. Creating synthetic dataset for demonstration...\")\n",
    "    \n",
    "    # Create synthetic AIS data\n",
    "    n_vessels = 100\n",
    "    n_timestamps = 1000\n",
    "    \n",
    "    vessels = [f\"VESSEL_{i:03d}\" for i in range(n_vessels)]\n",
    "    timestamps = pd.date_range('2023-01-01', periods=n_timestamps, freq='1H')\n",
    "    \n",
    "    data = []\n",
    "    for vessel in vessels:\n",
    "        # Simulate vessel trajectory\n",
    "        lat_base = np.random.uniform(40, 60)\n",
    "        lon_base = np.random.uniform(-10, 10)\n",
    "        \n",
    "        for i, ts in enumerate(timestamps):\n",
    "            # Add some trajectory variation\n",
    "            lat = lat_base + 0.1 * np.sin(i * 0.01) + np.random.normal(0, 0.01)\n",
    "            lon = lon_base + 0.1 * np.cos(i * 0.01) + np.random.normal(0, 0.01)\n",
    "            speed = np.random.uniform(5, 25)\n",
    "            heading = np.random.uniform(0, 360)\n",
    "            \n",
    "            data.append({\n",
    "                'mmsi': vessel,\n",
    "                'timestamp': ts,\n",
    "                'latitude': lat,\n",
    "                'longitude': lon,\n",
    "                'speed': speed,\n",
    "                'heading': heading\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(0.7 * len(timestamps))\n",
    "    val_size = int(0.15 * len(timestamps))\n",
    "    \n",
    "    train_data = df[df['timestamp'] < timestamps[train_size]]\n",
    "    val_data = df[(df['timestamp'] >= timestamps[train_size]) & \n",
    "                  (df['timestamp'] < timestamps[train_size + val_size])]\n",
    "    test_data = df[df['timestamp'] >= timestamps[train_size + val_size]]\n",
    "    \n",
    "    print(f\"Created synthetic data: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test samples\")\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"- Train: {train_data.shape}\")\n",
    "print(f\"- Validation: {val_data.shape}\")\n",
    "print(f\"- Test: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae926e",
   "metadata": {},
   "source": [
    "## 4. Model Configuration and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b881fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment configurations\n",
    "    \"tft_config_path = project_root / 'config' / 'experiment_configs' / 'experiment_tft.yaml'\n",
    "\",\n",
    "    \"nbeats_config_path = project_root / 'config' / 'experiment_configs' / 'experiment_nbeats.yaml'\n",
    "\",\n",
    "with open(tft_config_path, 'r') as f:\n",
    "    tft_config = yaml.safe_load(f)\n",
    "\n",
    "with open(nbeats_config_path, 'r') as f:\n",
    "    nbeats_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Model configurations loaded\")\n",
    "print(f\"TFT config: {tft_config['model']['model_type']}\")\n",
    "print(f\"N-BEATS config: {nbeats_config['model']['model_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e31aad",
   "metadata": {},
   "source": [
    "### 4.1 Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare time series sequences\n",
    "def prepare_sequences(data, sequence_length, prediction_horizon):\n",
    "    \"\"\"Prepare time series sequences for training.\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Group by vessel\n",
    "    for mmsi, group in data.groupby('mmsi'):\n",
    "        group = group.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        # Create sequences\n",
    "        for i in range(len(group) - sequence_length - prediction_horizon + 1):\n",
    "            seq = group.iloc[i:i+sequence_length][['latitude', 'longitude', 'speed', 'heading']].values\n",
    "            target = group.iloc[i+sequence_length:i+sequence_length+prediction_horizon][['latitude', 'longitude']].values\n",
    "            \n",
    "            sequences.append(seq)\n",
    "            targets.append(target)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "sequence_length = config['model']['sequence_length']\n",
    "prediction_horizon = config['model']['prediction_horizon']\n",
    "\n",
    "print(\"Preparing sequences...\")\n",
    "X_train, y_train = prepare_sequences(train_data, sequence_length, prediction_horizon)\n",
    "X_val, y_val = prepare_sequences(val_data, sequence_length, prediction_horizon)\n",
    "X_test, y_test = prepare_sequences(test_data, sequence_length, prediction_horizon)\n",
    "\n",
    "print(f\"Sequence shapes:\")\n",
    "print(f\"- X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"- X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"- X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e1cbbd",
   "metadata": {},
   "source": [
    "### 4.2 Train TFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d9bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training TFT Model...\")\n",
    "\n",
    "# Initialize TFT model\n",
    "tft_model = TFTModel(tft_config)\n",
    "\n",
    "# Prepare data loaders\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.FloatTensor(X_train), \n",
    "    torch.FloatTensor(y_train)\n",
    ")\n",
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.FloatTensor(X_val), \n",
    "    torch.FloatTensor(y_val)\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=tft_config['training']['batch_size'], \n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=tft_config['training']['batch_size'], \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Setup trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=tft_config['training']['epochs'],\n",
    "    accelerator='auto',\n",
    "    devices='auto',\n",
    "    deterministic=True,\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.fit(tft_model, train_loader, val_loader)\n",
    "\n",
    "print(\"TFT training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64055c6",
   "metadata": {},
   "source": [
    "### 4.3 Train N-BEATS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training N-BEATS Model...\")\n",
    "\n",
    "# Initialize N-BEATS model\n",
    "nbeats_model = NBeatsModel(nbeats_config)\n",
    "\n",
    "# Prepare data loaders with N-BEATS batch size\n",
    "train_loader_nbeats = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=nbeats_config['training']['batch_size'], \n",
    "    shuffle=True\n",
    ")\n",
    "val_loader_nbeats = torch.utils.data.DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=nbeats_config['training']['batch_size'], \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Setup trainer for N-BEATS\n",
    "trainer_nbeats = pl.Trainer(\n",
    "    max_epochs=nbeats_config['training']['epochs'],\n",
    "    accelerator='auto',\n",
    "    devices='auto',\n",
    "    deterministic=True,\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer_nbeats.fit(nbeats_model, train_loader_nbeats, val_loader_nbeats)\n",
    "\n",
    "print(\"N-BEATS training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd50de59",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe629ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models on test set\n",
    "print(\"Evaluating models on test set...\")\n",
    "\n",
    "# Prepare test data\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.FloatTensor(X_test), \n",
    "    torch.FloatTensor(y_test)\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get predictions from both models\n",
    "tft_predictions = trainer.predict(tft_model, test_loader)\n",
    "nbeats_predictions = trainer_nbeats.predict(nbeats_model, test_loader)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "tft_pred = torch.cat(tft_predictions).numpy()\n",
    "nbeats_pred = torch.cat(nbeats_predictions).numpy()\n",
    "y_true = y_test\n",
    "\n",
    "print(f\"Prediction shapes: TFT {tft_pred.shape}, N-BEATS {nbeats_pred.shape}, True {y_true.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8882258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for both models\n",
    "from src.utils.metrics import calculate_mae, calculate_rmse, calculate_smape\n",
    "\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(\"\\nTFT Model:\")\n",
    "tft_mae = calculate_mae(y_true, tft_pred)\n",
    "tft_rmse = calculate_rmse(y_true, tft_pred)\n",
    "tft_smape = calculate_smape(y_true, tft_pred)\n",
    "print(f\"  MAE: {tft_mae:.4f}\")\n",
    "print(f\"  RMSE: {tft_rmse:.4f}\")\n",
    "print(f\"  SMAPE: {tft_smape:.4f}\")\n",
    "\n",
    "print(\"\\nN-BEATS Model:\")\n",
    "nbeats_mae = calculate_mae(y_true, nbeats_pred)\n",
    "nbeats_rmse = calculate_rmse(y_true, nbeats_pred)\n",
    "nbeats_smape = calculate_smape(y_true, nbeats_pred)\n",
    "print(f\"  MAE: {nbeats_mae:.4f}\")\n",
    "print(f\"  RMSE: {nbeats_rmse:.4f}\")\n",
    "print(f\"  SMAPE: {nbeats_smape:.4f}\")\n",
    "\n",
    "# Determine best model\n",
    "best_model = \"TFT\" if tft_mae < nbeats_mae else \"N-BEATS\"\n",
    "print(f\"\\nBest model (by MAE): {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01972703",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e6aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample predictions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Model Predictions Comparison', fontsize=16)\n",
    "\n",
    "# Select a few samples for visualization\n",
    "sample_indices = [0, 5, 10, 15]\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices):\n",
    "    if sample_idx >= len(y_true):\n",
    "        continue\n",
    "        \n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Plot true trajectory\n",
    "    ax.plot(y_true[sample_idx, :, 1], y_true[sample_idx, :, 0], 'ko-', label='True', linewidth=2)\n",
    "    \n",
    "    # Plot TFT prediction\n",
    "    ax.plot(tft_pred[sample_idx, :, 1], tft_pred[sample_idx, :, 0], 'ro-', label='TFT', alpha=0.7)\n",
    "    \n",
    "    # Plot N-BEATS prediction\n",
    "    ax.plot(nbeats_pred[sample_idx, :, 1], nbeats_pred[sample_idx, :, 0], 'bo-', label='N-BEATS', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_title(f'Sample {sample_idx + 1}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a101b",
   "metadata": {},
   "source": [
    "## 7. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b32568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "models_dir = project_root / 'models'\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save TFT model\n",
    "tft_path = models_dir / 'tft_model.ckpt'\n",
    "trainer.save_checkpoint(tft_path)\n",
    "print(f\"TFT model saved to: {tft_path}\")\n",
    "\n",
    "# Save N-BEATS model\n",
    "nbeats_path = models_dir / 'nbeats_model.ckpt'\n",
    "trainer_nbeats.save_checkpoint(nbeats_path)\n",
    "print(f\"N-BEATS model saved to: {nbeats_path}\")\n",
    "\n",
    "# Save metrics comparison\n",
    "metrics_comparison = {\n",
    "    'TFT': {\n",
    "        'MAE': float(tft_mae),\n",
    "        'RMSE': float(tft_rmse),\n",
    "        'SMAPE': float(tft_smape)\n",
    "    },\n",
    "    'N-BEATS': {\n",
    "        'MAE': float(nbeats_mae),\n",
    "        'RMSE': float(nbeats_rmse),\n",
    "        'SMAPE': float(nbeats_smape)\n",
    "    },\n",
    "    'best_model': best_model\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(models_dir / 'metrics_comparison.json', 'w') as f:\n",
    "    json.dump(metrics_comparison, f, indent=2)\n",
    "\n",
    "print(\"\\nModel development completed!\")\n",
    "print(f\"Models and metrics saved to: {models_dir}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
