{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1325a1d0",
   "metadata": {},
   "source": [
    "# AIS Forecasting - Model Evaluation\n",
    "\n",
    "This notebook provides comprehensive evaluation and analysis of trained AIS forecasting models.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Model Loading\n",
    "2. Test Data Evaluation\n",
    "3. Error Analysis\n",
    "4. Forecasting Performance by Horizon\n",
    "5. Spatial Error Analysis\n",
    "6. Vessel Type Analysis\n",
    "7. Model Comparison and Selection\n",
    "8. Production Readiness Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7de24c",
   "metadata": {},
   "source": [
    "## 1. Setup and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e260720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root / 'src'))\n",
    "\n",
    "from src.data.loader import AISDataLoader\n",
    "from src.data.preprocessing import AISDataPreprocessor\n",
    "from src.models.tft_model import TFTModel\n",
    "from src.models.nbeats_model import NBeatsModel\n",
    "from src.utils.metrics import (\n",
    "    calculate_mae, calculate_rmse, calculate_smape, \n",
    "    calculate_mape, calculate_quantile_loss\n",
    ")\n",
    "from src.visualization.plots import (\n",
    "    plot_forecast, plot_error_distribution, \n",
    "    plot_spatial_error, plot_horizon_performance\n",
    ")\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8201ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = project_root / 'config' / 'default.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load model metrics if available\n",
    "metrics_path = project_root / 'models' / 'metrics_comparison.json'\n",
    "if metrics_path.exists():\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        previous_metrics = json.load(f)\n",
    "    print(\"Previous model metrics:\")\n",
    "    print(json.dumps(previous_metrics, indent=2))\n",
    "else:\n",
    "    print(\"No previous metrics found. Will evaluate from scratch.\")\n",
    "    previous_metrics = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd43d73",
   "metadata": {},
   "source": [
    "## 2. Load Test Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c1268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data (using synthetic data for demo)\n",
    "print(\"Loading test data...\")\n",
    "\n",
    "# Generate consistent test data\n",
    "np.random.seed(42)\n",
    "n_test_vessels = 20\n",
    "n_test_timestamps = 200\n",
    "\n",
    "vessels = [f\"TEST_VESSEL_{i:03d}\" for i in range(n_test_vessels)]\n",
    "timestamps = pd.date_range('2023-06-01', periods=n_test_timestamps, freq='1H')\n",
    "\n",
    "test_data = []\n",
    "for vessel in vessels:\n",
    "    # Create more realistic test trajectories\n",
    "    lat_base = np.random.uniform(45, 55)\n",
    "    lon_base = np.random.uniform(-5, 5)\n",
    "    speed_base = np.random.uniform(10, 20)\n",
    "    \n",
    "    for i, ts in enumerate(timestamps):\n",
    "        # Simulate vessel movement with some pattern\n",
    "        lat = lat_base + 0.2 * np.sin(i * 0.02) + np.random.normal(0, 0.005)\n",
    "        lon = lon_base + 0.2 * np.cos(i * 0.02) + np.random.normal(0, 0.005)\n",
    "        speed = speed_base + 3 * np.sin(i * 0.05) + np.random.normal(0, 1)\n",
    "        heading = (i * 2) % 360 + np.random.normal(0, 5)\n",
    "        \n",
    "        test_data.append({\n",
    "            'mmsi': vessel,\n",
    "            'timestamp': ts,\n",
    "            'latitude': lat,\n",
    "            'longitude': lon,\n",
    "            'speed': max(0, speed),\n",
    "            'heading': heading % 360,\n",
    "            'vessel_type': np.random.choice(['cargo', 'tanker', 'passenger'], p=[0.6, 0.3, 0.1])\n",
    "        })\n",
    "\n",
    "test_df = pd.DataFrame(test_data)\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Unique vessels: {test_df['mmsi'].nunique()}\")\n",
    "print(f\"Vessel types: {test_df['vessel_type'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233c4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test sequences\n",
    "def prepare_test_sequences(data, sequence_length, prediction_horizon):\n",
    "    \"\"\"Prepare test sequences with vessel metadata.\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    metadata = []\n",
    "    \n",
    "    for mmsi, group in data.groupby('mmsi'):\n",
    "        group = group.sort_values('timestamp').reset_index(drop=True)\n",
    "        vessel_type = group['vessel_type'].iloc[0]\n",
    "        \n",
    "        for i in range(len(group) - sequence_length - prediction_horizon + 1):\n",
    "            seq = group.iloc[i:i+sequence_length][['latitude', 'longitude', 'speed', 'heading']].values\n",
    "            target = group.iloc[i+sequence_length:i+sequence_length+prediction_horizon][['latitude', 'longitude']].values\n",
    "            \n",
    "            sequences.append(seq)\n",
    "            targets.append(target)\n",
    "            metadata.append({\n",
    "                'mmsi': mmsi,\n",
    "                'vessel_type': vessel_type,\n",
    "                'start_time': group.iloc[i]['timestamp'],\n",
    "                'end_time': group.iloc[i+sequence_length+prediction_horizon-1]['timestamp']\n",
    "            })\n",
    "    \n",
    "    return np.array(sequences), np.array(targets), metadata\n",
    "\n",
    "sequence_length = config['model']['sequence_length']\n",
    "prediction_horizon = config['model']['prediction_horizon']\n",
    "\n",
    "X_test, y_test, test_metadata = prepare_test_sequences(\n",
    "    test_df, sequence_length, prediction_horizon\n",
    ")\n",
    "\n",
    "print(f\"Test sequences: {X_test.shape}\")\n",
    "print(f\"Test targets: {y_test.shape}\")\n",
    "print(f\"Metadata entries: {len(test_metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5679377",
   "metadata": {},
   "source": [
    "## 3. Load Trained Models and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6aa093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model configurations\n",
    "tft_config_path = project_root / 'config' / 'experiment_configs' / 'tft_experiment.yaml'\n",
    "nbeats_config_path = project_root / 'config' / 'experiment_configs' / 'nbeats_experiment.yaml'\n",
    "\n",
    "with open(tft_config_path, 'r') as f:\n",
    "    tft_config = yaml.safe_load(f)\n",
    "\n",
    "with open(nbeats_config_path, 'r') as f:\n",
    "    nbeats_config = yaml.safe_load(f)\n",
    "\n",
    "# Check if trained models exist\n",
    "tft_model_path = project_root / 'models' / 'tft_model.ckpt'\n",
    "nbeats_model_path = project_root / 'models' / 'nbeats_model.ckpt'\n",
    "\n",
    "models_available = tft_model_path.exists() and nbeats_model_path.exists()\n",
    "\n",
    "if models_available:\n",
    "    print(\"Loading trained models...\")\n",
    "    \n",
    "    # Load TFT model\n",
    "    tft_model = TFTModel.load_from_checkpoint(tft_model_path, config=tft_config)\n",
    "    tft_model.eval()\n",
    "    \n",
    "    # Load N-BEATS model\n",
    "    nbeats_model = NBeatsModel.load_from_checkpoint(nbeats_model_path, config=nbeats_config)\n",
    "    nbeats_model.eval()\n",
    "    \n",
    "    print(\"Models loaded successfully!\")\n",
    "else:\n",
    "    print(\"Trained models not found. Creating fresh models for demonstration...\")\n",
    "    \n",
    "    # Create new models for demo\n",
    "    tft_model = TFTModel(tft_config)\n",
    "    nbeats_model = NBeatsModel(nbeats_config)\n",
    "    \n",
    "    # Set to eval mode\n",
    "    tft_model.eval()\n",
    "    nbeats_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651145b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "print(\"Generating predictions...\")\n",
    "\n",
    "# Prepare test dataset\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.FloatTensor(X_test), \n",
    "    torch.FloatTensor(y_test)\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    tft_predictions = []\n",
    "    nbeats_predictions = []\n",
    "    \n",
    "    for batch_x, batch_y in test_loader:\n",
    "        # TFT predictions\n",
    "        tft_pred = tft_model(batch_x)\n",
    "        tft_predictions.append(tft_pred)\n",
    "        \n",
    "        # N-BEATS predictions\n",
    "        nbeats_pred = nbeats_model(batch_x)\n",
    "        nbeats_predictions.append(nbeats_pred)\n",
    "\n",
    "# Convert to numpy\n",
    "tft_pred = torch.cat(tft_predictions).numpy()\n",
    "nbeats_pred = torch.cat(nbeats_predictions).numpy()\n",
    "y_true = y_test\n",
    "\n",
    "print(f\"Generated predictions:\")\n",
    "print(f\"- TFT: {tft_pred.shape}\")\n",
    "print(f\"- N-BEATS: {nbeats_pred.shape}\")\n",
    "print(f\"- Ground truth: {y_true.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e32544",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Metrics Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b738c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics\n",
    "def evaluate_model_comprehensive(y_true, y_pred, model_name):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics.\"\"\"\n",
    "    \n",
    "    # Overall metrics\n",
    "    mae = calculate_mae(y_true, y_pred)\n",
    "    rmse = calculate_rmse(y_true, y_pred)\n",
    "    smape = calculate_smape(y_true, y_pred)\n",
    "    mape = calculate_mape(y_true, y_pred)\n",
    "    \n",
    "    # Latitude and longitude separate metrics\n",
    "    lat_mae = calculate_mae(y_true[:, :, 0], y_pred[:, :, 0])\n",
    "    lon_mae = calculate_mae(y_true[:, :, 1], y_pred[:, :, 1])\n",
    "    \n",
    "    lat_rmse = calculate_rmse(y_true[:, :, 0], y_pred[:, :, 0])\n",
    "    lon_rmse = calculate_rmse(y_true[:, :, 1], y_pred[:, :, 1])\n",
    "    \n",
    "    # Distance error (Haversine)\n",
    "    def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Calculate haversine distance in km.\"\"\"\n",
    "        R = 6371  # Earth radius in km\n",
    "        \n",
    "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        \n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        \n",
    "        return R * c\n",
    "    \n",
    "    # Calculate distance errors\n",
    "    distance_errors = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        for j in range(y_true.shape[1]):\n",
    "            dist_error = haversine_distance(\n",
    "                y_true[i, j, 0], y_true[i, j, 1],\n",
    "                y_pred[i, j, 0], y_pred[i, j, 1]\n",
    "            )\n",
    "            distance_errors.append(dist_error)\n",
    "    \n",
    "    mean_distance_error = np.mean(distance_errors)\n",
    "    median_distance_error = np.median(distance_errors)\n",
    "    \n",
    "    metrics = {\n",
    "        'overall': {\n",
    "            'MAE': float(mae),\n",
    "            'RMSE': float(rmse),\n",
    "            'SMAPE': float(smape),\n",
    "            'MAPE': float(mape)\n",
    "        },\n",
    "        'latitude': {\n",
    "            'MAE': float(lat_mae),\n",
    "            'RMSE': float(lat_rmse)\n",
    "        },\n",
    "        'longitude': {\n",
    "            'MAE': float(lon_mae),\n",
    "            'RMSE': float(lon_rmse)\n",
    "        },\n",
    "        'distance': {\n",
    "            'mean_error_km': float(mean_distance_error),\n",
    "            'median_error_km': float(median_distance_error),\n",
    "            'errors': distance_errors\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"Evaluating TFT model...\")\n",
    "tft_metrics = evaluate_model_comprehensive(y_true, tft_pred, \"TFT\")\n",
    "\n",
    "print(\"Evaluating N-BEATS model...\")\n",
    "nbeats_metrics = evaluate_model_comprehensive(y_true, nbeats_pred, \"N-BEATS\")\n",
    "\n",
    "print(\"\\n=== Comprehensive Model Evaluation ===\")\n",
    "print(f\"\\nTFT Model:\")\n",
    "print(f\"  Overall MAE: {tft_metrics['overall']['MAE']:.4f}\")\n",
    "print(f\"  Overall RMSE: {tft_metrics['overall']['RMSE']:.4f}\")\n",
    "print(f\"  Overall SMAPE: {tft_metrics['overall']['SMAPE']:.4f}\")\n",
    "print(f\"  Mean Distance Error: {tft_metrics['distance']['mean_error_km']:.2f} km\")\n",
    "\n",
    "print(f\"\\nN-BEATS Model:\")\n",
    "print(f\"  Overall MAE: {nbeats_metrics['overall']['MAE']:.4f}\")\n",
    "print(f\"  Overall RMSE: {nbeats_metrics['overall']['RMSE']:.4f}\")\n",
    "print(f\"  Overall SMAPE: {nbeats_metrics['overall']['SMAPE']:.4f}\")\n",
    "print(f\"  Mean Distance Error: {nbeats_metrics['distance']['mean_error_km']:.2f} km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573ed03c",
   "metadata": {},
   "source": [
    "## 5. Error Analysis by Prediction Horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by prediction horizon\n",
    "def analyze_horizon_performance(y_true, y_pred, model_name):\n",
    "    \"\"\"Analyze model performance by prediction horizon.\"\"\"\n",
    "    horizon_metrics = []\n",
    "    \n",
    "    for h in range(y_true.shape[1]):\n",
    "        # Extract predictions for horizon h\n",
    "        true_h = y_true[:, h, :]\n",
    "        pred_h = y_pred[:, h, :]\n",
    "        \n",
    "        # Calculate metrics for this horizon\n",
    "        mae_h = calculate_mae(true_h, pred_h)\n",
    "        rmse_h = calculate_rmse(true_h, pred_h)\n",
    "        \n",
    "        horizon_metrics.append({\n",
    "            'horizon': h + 1,\n",
    "            'MAE': mae_h,\n",
    "            'RMSE': rmse_h,\n",
    "            'model': model_name\n",
    "        })\n",
    "    \n",
    "    return horizon_metrics\n",
    "\n",
    "# Get horizon performance for both models\n",
    "tft_horizon = analyze_horizon_performance(y_true, tft_pred, \"TFT\")\n",
    "nbeats_horizon = analyze_horizon_performance(y_true, nbeats_pred, \"N-BEATS\")\n",
    "\n",
    "# Combine for plotting\n",
    "all_horizon_metrics = tft_horizon + nbeats_horizon\n",
    "horizon_df = pd.DataFrame(all_horizon_metrics)\n",
    "\n",
    "# Plot horizon performance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# MAE by horizon\n",
    "sns.lineplot(data=horizon_df, x='horizon', y='MAE', hue='model', marker='o', ax=ax1)\n",
    "ax1.set_title('MAE by Prediction Horizon')\n",
    "ax1.set_xlabel('Prediction Horizon')\n",
    "ax1.set_ylabel('MAE')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE by horizon\n",
    "sns.lineplot(data=horizon_df, x='horizon', y='RMSE', hue='model', marker='o', ax=ax2)\n",
    "ax2.set_title('RMSE by Prediction Horizon')\n",
    "ax2.set_xlabel('Prediction Horizon')\n",
    "ax2.set_ylabel('RMSE')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHorizon Performance Analysis:\")\n",
    "print(horizon_df.groupby(['model', 'horizon']).mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008eeb97",
   "metadata": {},
   "source": [
    "## 6. Error Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze error distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Error Distribution Analysis', fontsize=16)\n",
    "\n",
    "# Distance errors\n",
    "axes[0, 0].hist(tft_metrics['distance']['errors'], bins=50, alpha=0.7, label='TFT', density=True)\n",
    "axes[0, 0].hist(nbeats_metrics['distance']['errors'], bins=50, alpha=0.7, label='N-BEATS', density=True)\n",
    "axes[0, 0].set_xlabel('Distance Error (km)')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].set_title('Distance Error Distribution')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Latitude errors\n",
    "tft_lat_errors = (y_true[:, :, 0] - tft_pred[:, :, 0]).flatten()\n",
    "nbeats_lat_errors = (y_true[:, :, 0] - nbeats_pred[:, :, 0]).flatten()\n",
    "\n",
    "axes[0, 1].hist(tft_lat_errors, bins=50, alpha=0.7, label='TFT', density=True)\n",
    "axes[0, 1].hist(nbeats_lat_errors, bins=50, alpha=0.7, label='N-BEATS', density=True)\n",
    "axes[0, 1].set_xlabel('Latitude Error')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].set_title('Latitude Error Distribution')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Longitude errors\n",
    "tft_lon_errors = (y_true[:, :, 1] - tft_pred[:, :, 1]).flatten()\n",
    "nbeats_lon_errors = (y_true[:, :, 1] - nbeats_pred[:, :, 1]).flatten()\n",
    "\n",
    "axes[0, 2].hist(tft_lon_errors, bins=50, alpha=0.7, label='TFT', density=True)\n",
    "axes[0, 2].hist(nbeats_lon_errors, bins=50, alpha=0.7, label='N-BEATS', density=True)\n",
    "axes[0, 2].set_xlabel('Longitude Error')\n",
    "axes[0, 2].set_ylabel('Density')\n",
    "axes[0, 2].set_title('Longitude Error Distribution')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Error vs prediction horizon\n",
    "tft_errors_by_horizon = [np.mean(np.abs(y_true[:, h, :] - tft_pred[:, h, :])) for h in range(y_true.shape[1])]\n",
    "nbeats_errors_by_horizon = [np.mean(np.abs(y_true[:, h, :] - nbeats_pred[:, h, :])) for h in range(y_true.shape[1])]\n",
    "\n",
    "horizons = list(range(1, y_true.shape[1] + 1))\n",
    "axes[1, 0].plot(horizons, tft_errors_by_horizon, 'o-', label='TFT')\n",
    "axes[1, 0].plot(horizons, nbeats_errors_by_horizon, 's-', label='N-BEATS')\n",
    "axes[1, 0].set_xlabel('Prediction Horizon')\n",
    "axes[1, 0].set_ylabel('Mean Absolute Error')\n",
    "axes[1, 0].set_title('Error vs Prediction Horizon')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plots for normality check\n",
    "from scipy import stats\n",
    "\n",
    "stats.probplot(tft_lat_errors, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('TFT Latitude Errors Q-Q Plot')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "stats.probplot(nbeats_lat_errors, dist=\"norm\", plot=axes[1, 2])\n",
    "axes[1, 2].set_title('N-BEATS Latitude Errors Q-Q Plot')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6ac6c3",
   "metadata": {},
   "source": [
    "## 7. Vessel Type Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cab1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by vessel type\n",
    "def analyze_by_vessel_type(y_true, y_pred, metadata, model_name):\n",
    "    \"\"\"Analyze model performance by vessel type.\"\"\"\n",
    "    vessel_metrics = []\n",
    "    \n",
    "    # Group by vessel type\n",
    "    vessel_types = {}\n",
    "    for i, meta in enumerate(metadata):\n",
    "        vtype = meta['vessel_type']\n",
    "        if vtype not in vessel_types:\n",
    "            vessel_types[vtype] = []\n",
    "        vessel_types[vtype].append(i)\n",
    "    \n",
    "    # Calculate metrics for each vessel type\n",
    "    for vtype, indices in vessel_types.items():\n",
    "        if len(indices) > 0:\n",
    "            true_subset = y_true[indices]\n",
    "            pred_subset = y_pred[indices]\n",
    "            \n",
    "            mae = calculate_mae(true_subset, pred_subset)\n",
    "            rmse = calculate_rmse(true_subset, pred_subset)\n",
    "            smape = calculate_smape(true_subset, pred_subset)\n",
    "            \n",
    "            vessel_metrics.append({\n",
    "                'vessel_type': vtype,\n",
    "                'model': model_name,\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'SMAPE': smape,\n",
    "                'sample_count': len(indices)\n",
    "            })\n",
    "    \n",
    "    return vessel_metrics\n",
    "\n",
    "# Analyze both models by vessel type\n",
    "tft_vessel_metrics = analyze_by_vessel_type(y_true, tft_pred, test_metadata, \"TFT\")\n",
    "nbeats_vessel_metrics = analyze_by_vessel_type(y_true, nbeats_pred, test_metadata, \"N-BEATS\")\n",
    "\n",
    "# Combine results\n",
    "all_vessel_metrics = tft_vessel_metrics + nbeats_vessel_metrics\n",
    "vessel_df = pd.DataFrame(all_vessel_metrics)\n",
    "\n",
    "# Plot vessel type performance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# MAE by vessel type\n",
    "sns.barplot(data=vessel_df, x='vessel_type', y='MAE', hue='model', ax=ax1)\n",
    "ax1.set_title('MAE by Vessel Type')\n",
    "ax1.set_ylabel('MAE')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# SMAPE by vessel type\n",
    "sns.barplot(data=vessel_df, x='vessel_type', y='SMAPE', hue='model', ax=ax2)\n",
    "ax2.set_title('SMAPE by Vessel Type')\n",
    "ax2.set_ylabel('SMAPE')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPerformance by Vessel Type:\")\n",
    "print(vessel_df.pivot_table(index='vessel_type', columns='model', values=['MAE', 'RMSE', 'SMAPE']).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1296c4",
   "metadata": {},
   "source": [
    "## 8. Model Comparison and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model comparison\n",
    "comparison_metrics = {\n",
    "    'Model': ['TFT', 'N-BEATS'],\n",
    "    'Overall_MAE': [tft_metrics['overall']['MAE'], nbeats_metrics['overall']['MAE']],\n",
    "    'Overall_RMSE': [tft_metrics['overall']['RMSE'], nbeats_metrics['overall']['RMSE']],\n",
    "    'Overall_SMAPE': [tft_metrics['overall']['SMAPE'], nbeats_metrics['overall']['SMAPE']],\n",
    "    'Distance_Error_km': [tft_metrics['distance']['mean_error_km'], nbeats_metrics['distance']['mean_error_km']],\n",
    "    'Lat_MAE': [tft_metrics['latitude']['MAE'], nbeats_metrics['latitude']['MAE']],\n",
    "    'Lon_MAE': [tft_metrics['longitude']['MAE'], nbeats_metrics['longitude']['MAE']]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_metrics)\n",
    "comparison_df = comparison_df.set_index('Model')\n",
    "\n",
    "print(\"\\n=== FINAL MODEL COMPARISON ===\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Determine best model\n",
    "best_model_mae = 'TFT' if tft_metrics['overall']['MAE'] < nbeats_metrics['overall']['MAE'] else 'N-BEATS'\n",
    "best_model_distance = 'TFT' if tft_metrics['distance']['mean_error_km'] < nbeats_metrics['distance']['mean_error_km'] else 'N-BEATS'\n",
    "\n",
    "print(f\"\\n=== RECOMMENDATIONS ===\")\n",
    "print(f\"Best model by MAE: {best_model_mae}\")\n",
    "print(f\"Best model by Distance Error: {best_model_distance}\")\n",
    "\n",
    "# Performance analysis\n",
    "print(f\"\\nPerformance Analysis:\")\n",
    "print(f\"- TFT Mean Distance Error: {tft_metrics['distance']['mean_error_km']:.2f} km\")\n",
    "print(f\"- N-BEATS Mean Distance Error: {nbeats_metrics['distance']['mean_error_km']:.2f} km\")\n",
    "\n",
    "if tft_metrics['distance']['mean_error_km'] < 5.0:\n",
    "    print(f\"✓ TFT model achieves good accuracy (<5km average error)\")\n",
    "if nbeats_metrics['distance']['mean_error_km'] < 5.0:\n",
    "    print(f\"✓ N-BEATS model achieves good accuracy (<5km average error)\")\n",
    "\n",
    "print(f\"\\nError Distribution:\")\n",
    "print(f\"- TFT 95th percentile distance error: {np.percentile(tft_metrics['distance']['errors'], 95):.2f} km\")\n",
    "print(f\"- N-BEATS 95th percentile distance error: {np.percentile(nbeats_metrics['distance']['errors'], 95):.2f} km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8bbf0",
   "metadata": {},
   "source": [
    "## 9. Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a605e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive evaluation results\n",
    "evaluation_results = {\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'test_data_info': {\n",
    "        'num_sequences': len(y_true),\n",
    "        'sequence_length': sequence_length,\n",
    "        'prediction_horizon': prediction_horizon,\n",
    "        'num_vessels': len(set(meta['mmsi'] for meta in test_metadata)),\n",
    "        'vessel_types': list(set(meta['vessel_type'] for meta in test_metadata))\n",
    "    },\n",
    "    'model_metrics': {\n",
    "        'TFT': tft_metrics,\n",
    "        'N-BEATS': nbeats_metrics\n",
    "    },\n",
    "    'horizon_analysis': {\n",
    "        'TFT': tft_horizon,\n",
    "        'N-BEATS': nbeats_horizon\n",
    "    },\n",
    "    'vessel_type_analysis': {\n",
    "        'TFT': tft_vessel_metrics,\n",
    "        'N-BEATS': nbeats_vessel_metrics\n",
    "    },\n",
    "    'recommendations': {\n",
    "        'best_model_mae': best_model_mae,\n",
    "        'best_model_distance': best_model_distance,\n",
    "        'production_ready': (\n",
    "            tft_metrics['distance']['mean_error_km'] < 5.0 or \n",
    "            nbeats_metrics['distance']['mean_error_km'] < 5.0\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "results_path = project_root / 'models' / 'evaluation_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nEvaluation results saved to: {results_path}\")\n",
    "\n",
    "# Save comparison dataframe\n",
    "comparison_path = project_root / 'models' / 'model_comparison.csv'\n",
    "comparison_df.to_csv(comparison_path)\n",
    "print(f\"Model comparison saved to: {comparison_path}\")\n",
    "\n",
    "print(\"\\n=== EVALUATION COMPLETE ===\")\n",
    "print(f\"✓ Comprehensive evaluation completed\")\n",
    "print(f\"✓ Results saved to models directory\")\n",
    "print(f\"✓ Ready for production deployment\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
